{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "   \"torch\",\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, List\n",
    "import torch\n",
    "from torch import nn\n",
    "from labml import tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- regarding `def foward(x)`:\n",
    "    - Input has shape `[seq_len, batch_size, d_model]` or `[batch_size, d_model]`. We apply the linear transformation to the last dimension and split that into the heads.\n",
    "- Output shape `return x` has `[seq_len, batch_size, heads, d_k] or [batch_size, heads, d_model]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareForMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, heads: int, d_k: int, bias: bool):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, heads * d_k, bias=bias)\n",
    "        self.heads = heads\n",
    "        self.d_k = d_k\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        head_shape = x.shape[:-1]\n",
    "        x = self.linear(x) # Linear transformation\n",
    "        x = x.view(*head_shape, self.heads, self.d_k) # Split last dimensions into heads\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention Module\n",
    "- this computes scaled multi-head attention for a given `query`, `key` and `value` vectors\n",
    "- In simple terms: It finds the key that matches the query, and gets the value of those keys\n",
    "- It uses dot product of query and key as the indicator of how matching they are\n",
    "- Before taking the softmax the dot-products are scaled by $\\frac{1}{\\sqrt{d_k}}$\n",
    "- This is done to avoid large dot-product values causing softmax to give very small gradients when $d_k$ is large\n",
    "- Softmax is calculated along the axis of the sequence (or time)\n",
    "- regarding `def get_scores(query, key)`:\n",
    "    - This calculates $QK^T$\n",
    "    - but this method can also be overriden for other variations like relative attention\n",
    "- regarding `def prepare_mask(mask, query_shape, key_shape)`:\n",
    "    - mask has shape `[seq_len_q, seq_len_k, batch_size]` , where first dimension is the query dimension. If the query dimension is equal to 1 it will be broadcasted\n",
    "    - resulting mask has shape `[seq_len_q, seq_len_k, batch_size, heads]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // heads # Number of features per head\n",
    "        self.heads = heads\n",
    "        # These transform the query, key and value vectors for multi-head attention\n",
    "        self.query = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n",
    "        self.key = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n",
    "        self.value = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=True)\n",
    "        self.softmax = nn.Softmax(dim=1) # Softmax for attention along the time dimension of 'key'\n",
    "        self.output = nn.Linear(d_model, d_model) # Output layer\n",
    "        self.dropout = nn.Dropout(dropout_prob) # Dropout\n",
    "        self.scale = 1 / math.sqrt(self.d_k) # Scaling factor before the softmax\n",
    "        # We store the attentions so that it can be used for logging, or other computations if needed\n",
    "        self.attn = None\n",
    "\n",
    "    def get_scores(self, query: torch.Tensor, key: torch.Tensor):\n",
    "        return torch.einsum('ibhd,jbhd->ijbh', query, key)\n",
    "    \n",
    "    def prepare_mask(self, mask: torch.Tensor, query_shape: List[int], key_shape: List[int]):\n",
    "        assert mask.shape[0] == 1 or mask.shape[0] == query_shape[0]\n",
    "        assert mask.shape[1] == key_shape[0]\n",
    "        assert mask.shape[2] == 1 or mask.shape[2] == query_shape[1]\n",
    "\n",
    "        mask = mask.unsqueeze(-1) # Same mask applied to all heads\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, *, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        seq_len, batch_size, _ = query.shape # query , key and value have shape [seq_len, batch_size, d_model]\n",
    "        if mask is not None:\n",
    "            mask = self.prepare_mask(mask, query.shape, key.shape)\n",
    "        \n",
    "        # Prepare query, key and value for attention computation. These will then have shape [seq_len, batch_size, heads, d_k]\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "\n",
    "        # Compute the attention scores QK^T. This gives a tensor of shape [seq_len, seq_len, batch_size, heads]\n",
    "        scores = self.get_scores(query, key)\n",
    "        scores *= self.scale # Scaling the scores\n",
    "\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Softmax attention along the key sequence dimension\n",
    "        attn = self.softmax(scores)\n",
    "\n",
    "        # Save attentions if debugging\n",
    "        tracker.save('attn', attn)\n",
    "\n",
    "        # Apply Dropout\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        x = torch.einsum(\"ijbh,jbhd->ibhd\", attn, value)\n",
    "\n",
    "        # Save attentions for any other calculations\n",
    "        self.attn = attn.detach()\n",
    "\n",
    "        # Concatenate multiple heads\n",
    "        x = x.reshape(seq_len, batch_size, -1)\n",
    "\n",
    "        return self.output(x) # Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 32\n",
    "dropout_prob = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = MultiHeadAttention(heads=heads, d_model=d_model, dropout_prob=dropout_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = torch.rand(seq_len, batch_size, d_model) # Shape: [seq_len, batch_size, d_model]\n",
    "key = torch.rand(seq_len, batch_size, d_model) # Shape: [seq_len, batch_size, d_model]\n",
    "value = torch.rand(seq_len, batch_size, d_model) # Shape: [seq_len, batch_size, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones(1, seq_len, seq_len) # Optional mask (shape: [batch_size, seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\"><span style=\"color: #C5C1B4\"></span>\n",
       "<span style=\"color: #C5C1B4\">--------------------------------------------------</span><span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\"></span></strong></span>\n",
       "<span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\">LABML WARNING</span></strong></span>\n",
       "<span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\"></span></strong></span><strong>.labml.yaml</strong> config file could not be found. Looking in path: <span style=\"color: #208FFB\">/Users/makarwuckert/Desktop/papers/mha</span><span style=\"color: #C5C1B4\"></span>\n",
       "<span style=\"color: #C5C1B4\">--------------------------------------------------</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = attention_layer(query=query, key=key, value=value) # forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor: tensor([[[ 1.1527e-01, -1.6255e-02,  2.8769e-01,  ..., -1.0958e-01,\n",
      "          -3.6222e-02, -4.0379e-01],\n",
      "         [ 1.5179e-01,  5.0330e-03,  3.7713e-01,  ..., -1.5217e-01,\n",
      "           1.4615e-02, -4.6572e-01],\n",
      "         [ 1.5485e-01, -1.2671e-02,  3.3475e-01,  ..., -5.5276e-02,\n",
      "          -3.3743e-03, -4.8023e-01],\n",
      "         ...,\n",
      "         [ 1.2708e-01,  1.7308e-02,  3.3037e-01,  ..., -1.2917e-01,\n",
      "          -5.0667e-02, -4.5092e-01],\n",
      "         [ 1.1918e-01, -1.8682e-02,  3.6012e-01,  ..., -1.5683e-01,\n",
      "           3.9064e-02, -3.9264e-01],\n",
      "         [ 2.0257e-01,  3.5321e-02,  3.5771e-01,  ..., -1.5535e-01,\n",
      "          -7.4849e-03, -5.5601e-01]],\n",
      "\n",
      "        [[ 1.6778e-01, -1.8294e-02,  3.7324e-01,  ..., -1.3120e-01,\n",
      "          -5.4561e-03, -4.4727e-01],\n",
      "         [ 1.4385e-01, -7.2789e-03,  3.6052e-01,  ..., -1.4180e-01,\n",
      "          -7.4540e-03, -4.5073e-01],\n",
      "         [ 1.6232e-01,  2.5196e-02,  3.5972e-01,  ..., -1.1105e-01,\n",
      "          -5.0964e-03, -4.5416e-01],\n",
      "         ...,\n",
      "         [ 8.9604e-02,  2.0946e-03,  3.4842e-01,  ..., -1.2068e-01,\n",
      "          -7.9166e-02, -4.7003e-01],\n",
      "         [ 1.0625e-01, -2.0035e-02,  3.4570e-01,  ..., -1.3411e-01,\n",
      "           3.6882e-02, -3.8152e-01],\n",
      "         [ 1.7973e-01,  2.7835e-02,  3.8236e-01,  ..., -1.4972e-01,\n",
      "          -3.6837e-02, -5.5333e-01]],\n",
      "\n",
      "        [[ 1.5706e-01,  5.4292e-03,  3.6922e-01,  ..., -1.2836e-01,\n",
      "           7.9327e-03, -4.8751e-01],\n",
      "         [ 1.3056e-01, -4.7719e-03,  3.3426e-01,  ..., -1.0837e-01,\n",
      "           1.3109e-02, -3.9687e-01],\n",
      "         [ 1.7013e-01, -3.1837e-02,  3.8144e-01,  ..., -1.1250e-01,\n",
      "          -1.6025e-02, -4.5713e-01],\n",
      "         ...,\n",
      "         [ 9.7343e-02, -2.5408e-02,  2.6757e-01,  ..., -5.5566e-02,\n",
      "          -3.5876e-02, -3.9050e-01],\n",
      "         [ 1.5811e-01, -4.2851e-02,  3.7038e-01,  ..., -1.5382e-01,\n",
      "           8.3825e-02, -3.9925e-01],\n",
      "         [ 1.6597e-01,  1.9518e-02,  3.3076e-01,  ..., -1.2964e-01,\n",
      "          -3.0435e-02, -5.4388e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.4029e-01, -1.7805e-02,  3.5963e-01,  ..., -1.2213e-01,\n",
      "          -2.3542e-02, -4.4138e-01],\n",
      "         [ 1.5864e-01,  1.8533e-02,  3.7430e-01,  ..., -1.5041e-01,\n",
      "          -5.2303e-03, -4.6645e-01],\n",
      "         [ 1.4641e-01, -3.2189e-02,  3.4275e-01,  ..., -7.0169e-02,\n",
      "          -3.5098e-03, -4.4944e-01],\n",
      "         ...,\n",
      "         [ 7.7974e-02, -1.3683e-02,  3.2336e-01,  ..., -8.7965e-02,\n",
      "          -5.4490e-02, -4.3613e-01],\n",
      "         [ 1.4339e-01, -3.8050e-02,  3.7538e-01,  ..., -1.5276e-01,\n",
      "           5.5208e-02, -4.0568e-01],\n",
      "         [ 1.6899e-01,  3.2458e-02,  3.1914e-01,  ..., -1.0205e-01,\n",
      "          -2.6560e-02, -5.2690e-01]],\n",
      "\n",
      "        [[ 1.6319e-01, -5.8207e-03,  3.7115e-01,  ..., -1.4634e-01,\n",
      "           1.2471e-02, -4.6559e-01],\n",
      "         [ 1.2775e-01,  8.4672e-03,  3.5394e-01,  ..., -1.4564e-01,\n",
      "          -1.1475e-02, -4.3497e-01],\n",
      "         [ 1.5863e-01, -4.5332e-02,  3.6759e-01,  ..., -6.7735e-02,\n",
      "           3.8990e-03, -4.7118e-01],\n",
      "         ...,\n",
      "         [ 8.2871e-02, -9.2625e-03,  3.4735e-01,  ..., -1.0692e-01,\n",
      "          -6.0536e-02, -4.7174e-01],\n",
      "         [ 1.1606e-01, -3.2365e-02,  3.3247e-01,  ..., -1.0791e-01,\n",
      "           2.4585e-02, -3.7418e-01],\n",
      "         [ 1.3257e-01,  4.1094e-02,  3.0029e-01,  ..., -1.1671e-01,\n",
      "          -5.9727e-02, -5.3343e-01]],\n",
      "\n",
      "        [[ 1.4382e-01, -3.6442e-02,  3.5977e-01,  ..., -1.1412e-01,\n",
      "           1.6790e-02, -4.3348e-01],\n",
      "         [ 1.7702e-01, -8.9679e-03,  3.6126e-01,  ..., -1.2460e-01,\n",
      "           1.5206e-03, -4.5371e-01],\n",
      "         [ 1.6284e-01, -6.8603e-02,  3.9672e-01,  ..., -1.0049e-01,\n",
      "          -4.3145e-04, -4.3378e-01],\n",
      "         ...,\n",
      "         [ 7.2784e-02, -9.3610e-05,  3.2249e-01,  ..., -1.0223e-01,\n",
      "          -3.7684e-02, -4.3514e-01],\n",
      "         [ 1.2458e-01, -3.9530e-03,  3.4433e-01,  ..., -1.3471e-01,\n",
      "           5.7535e-02, -4.0067e-01],\n",
      "         [ 1.8868e-01, -6.5031e-04,  3.4321e-01,  ..., -1.2323e-01,\n",
      "          -3.4667e-02, -5.1919e-01]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Output tensor: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Output shape: {output.shape}\")\n",
    "assert output.shape == (seq_len, batch_size, d_model), \"Output shape is incorrect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights shape: torch.Size([10, 10, 32, 8])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Attention weights shape: {attention_layer.attn.shape}\")\n",
    "assert attention_layer.attn.shape == (seq_len, seq_len, batch_size, heads), \"Attention weights shape is incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1115, 0.0000, 0.1163,  ..., 0.0000, 0.1117, 0.1007],\n",
       "          [0.1051, 0.1136, 0.1143,  ..., 0.1055, 0.1051, 0.0000],\n",
       "          [0.1088, 0.1030, 0.1058,  ..., 0.0989, 0.1057, 0.1238],\n",
       "          ...,\n",
       "          [0.1093, 0.1166, 0.1056,  ..., 0.1082, 0.1077, 0.1142],\n",
       "          [0.0000, 0.1061, 0.1129,  ..., 0.1095, 0.1148, 0.1028],\n",
       "          [0.1082, 0.1052, 0.1152,  ..., 0.1174, 0.1093, 0.1117]],\n",
       "\n",
       "         [[0.1139, 0.1099, 0.1098,  ..., 0.0000, 0.1051, 0.1115],\n",
       "          [0.0000, 0.1126, 0.1082,  ..., 0.1259, 0.1118, 0.1125],\n",
       "          [0.0000, 0.1064, 0.1094,  ..., 0.1041, 0.1081, 0.1045],\n",
       "          ...,\n",
       "          [0.1023, 0.1173, 0.1126,  ..., 0.0000, 0.1105, 0.1189],\n",
       "          [0.0000, 0.1069, 0.1030,  ..., 0.1078, 0.1074, 0.0998],\n",
       "          [0.1177, 0.1076, 0.1156,  ..., 0.1071, 0.0000, 0.1123]],\n",
       "\n",
       "         [[0.1095, 0.1068, 0.1157,  ..., 0.1267, 0.1241, 0.1117],\n",
       "          [0.1135, 0.1086, 0.1077,  ..., 0.1128, 0.1172, 0.0000],\n",
       "          [0.1058, 0.1121, 0.1114,  ..., 0.0000, 0.1077, 0.1144],\n",
       "          ...,\n",
       "          [0.1152, 0.1061, 0.1160,  ..., 0.1093, 0.1142, 0.1141],\n",
       "          [0.1145, 0.1111, 0.1165,  ..., 0.1100, 0.1081, 0.1168],\n",
       "          [0.1155, 0.1098, 0.1168,  ..., 0.1163, 0.1144, 0.1091]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.1062, 0.1290, 0.1064,  ..., 0.1036, 0.1224, 0.1187],\n",
       "          [0.1089, 0.1121, 0.0000,  ..., 0.1176, 0.1099, 0.1099],\n",
       "          [0.1030, 0.1084, 0.1003,  ..., 0.1242, 0.1052, 0.1088],\n",
       "          ...,\n",
       "          [0.1106, 0.1083, 0.1080,  ..., 0.1131, 0.1102, 0.0000],\n",
       "          [0.1036, 0.1126, 0.1097,  ..., 0.1187, 0.1163, 0.1184],\n",
       "          [0.1024, 0.1167, 0.1118,  ..., 0.1006, 0.1186, 0.0000]],\n",
       "\n",
       "         [[0.1182, 0.1111, 0.1117,  ..., 0.0000, 0.1026, 0.1069],\n",
       "          [0.1161, 0.1121, 0.1125,  ..., 0.1050, 0.1105, 0.1165],\n",
       "          [0.1153, 0.1177, 0.1161,  ..., 0.1179, 0.1071, 0.0977],\n",
       "          ...,\n",
       "          [0.1112, 0.1111, 0.1100,  ..., 0.1063, 0.1190, 0.1102],\n",
       "          [0.1149, 0.1171, 0.1023,  ..., 0.1086, 0.1115, 0.1048],\n",
       "          [0.1148, 0.1131, 0.1127,  ..., 0.1220, 0.1227, 0.1036]],\n",
       "\n",
       "         [[0.1070, 0.1063, 0.0000,  ..., 0.1209, 0.1111, 0.1133],\n",
       "          [0.1144, 0.1068, 0.1134,  ..., 0.1108, 0.1142, 0.1116],\n",
       "          [0.1057, 0.1091, 0.1223,  ..., 0.1089, 0.1330, 0.1035],\n",
       "          ...,\n",
       "          [0.1053, 0.1044, 0.1195,  ..., 0.1133, 0.1132, 0.0000],\n",
       "          [0.1030, 0.1090, 0.1187,  ..., 0.1229, 0.0000, 0.1259],\n",
       "          [0.1135, 0.1123, 0.1028,  ..., 0.1140, 0.1089, 0.1194]]],\n",
       "\n",
       "\n",
       "        [[[0.1160, 0.1126, 0.1134,  ..., 0.1101, 0.0000, 0.1004],\n",
       "          [0.1125, 0.1167, 0.1182,  ..., 0.1086, 0.0000, 0.1140],\n",
       "          [0.0000, 0.1083, 0.1122,  ..., 0.0978, 0.1069, 0.1194],\n",
       "          ...,\n",
       "          [0.1068, 0.1205, 0.1009,  ..., 0.1049, 0.1076, 0.1128],\n",
       "          [0.1228, 0.1078, 0.0000,  ..., 0.0000, 0.1201, 0.1051],\n",
       "          [0.0000, 0.1026, 0.1140,  ..., 0.1114, 0.1075, 0.1090]],\n",
       "\n",
       "         [[0.1074, 0.1090, 0.1063,  ..., 0.0000, 0.1007, 0.1132],\n",
       "          [0.1051, 0.1166, 0.1123,  ..., 0.1182, 0.1052, 0.1084],\n",
       "          [0.1129, 0.0979, 0.1071,  ..., 0.1055, 0.1123, 0.0000],\n",
       "          ...,\n",
       "          [0.1074, 0.1145, 0.1160,  ..., 0.1089, 0.1193, 0.1172],\n",
       "          [0.1033, 0.1092, 0.1007,  ..., 0.1128, 0.1098, 0.0990],\n",
       "          [0.0000, 0.1109, 0.1064,  ..., 0.1067, 0.1079, 0.1075]],\n",
       "\n",
       "         [[0.0000, 0.1094, 0.1184,  ..., 0.1245, 0.1222, 0.0000],\n",
       "          [0.1070, 0.1073, 0.1104,  ..., 0.1097, 0.1176, 0.1031],\n",
       "          [0.1112, 0.1113, 0.1108,  ..., 0.0000, 0.1123, 0.1132],\n",
       "          ...,\n",
       "          [0.1081, 0.0000, 0.1099,  ..., 0.1118, 0.1114, 0.0000],\n",
       "          [0.1173, 0.1167, 0.1150,  ..., 0.1077, 0.1030, 0.1139],\n",
       "          [0.1178, 0.1146, 0.1130,  ..., 0.1157, 0.1194, 0.1137]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0000, 0.0000, 0.1069,  ..., 0.1046, 0.1223, 0.1136],\n",
       "          [0.1103, 0.1171, 0.1197,  ..., 0.1189, 0.1128, 0.1118],\n",
       "          [0.1038, 0.1123, 0.1009,  ..., 0.1199, 0.1022, 0.1046],\n",
       "          ...,\n",
       "          [0.1019, 0.1142, 0.1117,  ..., 0.1120, 0.1029, 0.1169],\n",
       "          [0.1070, 0.1091, 0.1151,  ..., 0.1106, 0.1107, 0.1156],\n",
       "          [0.1037, 0.1165, 0.1063,  ..., 0.1083, 0.1196, 0.1134]],\n",
       "\n",
       "         [[0.1227, 0.1103, 0.1129,  ..., 0.0993, 0.1082, 0.1078],\n",
       "          [0.1227, 0.1121, 0.1114,  ..., 0.1034, 0.1085, 0.1119],\n",
       "          [0.1111, 0.1165, 0.0000,  ..., 0.1165, 0.1048, 0.1038],\n",
       "          ...,\n",
       "          [0.1183, 0.1117, 0.1120,  ..., 0.1051, 0.1220, 0.1049],\n",
       "          [0.1035, 0.1180, 0.1071,  ..., 0.1073, 0.1110, 0.1136],\n",
       "          [0.1082, 0.1128, 0.1144,  ..., 0.1171, 0.1196, 0.1110]],\n",
       "\n",
       "         [[0.1019, 0.1074, 0.0000,  ..., 0.1169, 0.1141, 0.1126],\n",
       "          [0.1180, 0.0000, 0.1113,  ..., 0.1148, 0.1167, 0.1103],\n",
       "          [0.1109, 0.1090, 0.1188,  ..., 0.1032, 0.1302, 0.1055],\n",
       "          ...,\n",
       "          [0.1038, 0.1013, 0.1191,  ..., 0.1170, 0.1066, 0.1045],\n",
       "          [0.1080, 0.1058, 0.1187,  ..., 0.1227, 0.1042, 0.1253],\n",
       "          [0.1124, 0.1104, 0.1094,  ..., 0.1141, 0.0000, 0.1225]]],\n",
       "\n",
       "\n",
       "        [[[0.1108, 0.0000, 0.1127,  ..., 0.1163, 0.1115, 0.1029],\n",
       "          [0.1023, 0.1178, 0.0000,  ..., 0.1130, 0.1034, 0.1182],\n",
       "          [0.1154, 0.1048, 0.1161,  ..., 0.1080, 0.1073, 0.1259],\n",
       "          ...,\n",
       "          [0.1011, 0.1235, 0.1060,  ..., 0.1049, 0.1074, 0.1086],\n",
       "          [0.1155, 0.1096, 0.1136,  ..., 0.1110, 0.1203, 0.1047],\n",
       "          [0.1120, 0.1056, 0.1121,  ..., 0.1120, 0.1037, 0.1146]],\n",
       "\n",
       "         [[0.1140, 0.1048, 0.1151,  ..., 0.1081, 0.1037, 0.1137],\n",
       "          [0.1122, 0.1132, 0.1110,  ..., 0.1175, 0.1074, 0.0000],\n",
       "          [0.1192, 0.1081, 0.1086,  ..., 0.1048, 0.1149, 0.1073],\n",
       "          ...,\n",
       "          [0.1025, 0.1132, 0.1143,  ..., 0.1108, 0.1117, 0.1124],\n",
       "          [0.1144, 0.1014, 0.1087,  ..., 0.1043, 0.1065, 0.1018],\n",
       "          [0.1132, 0.1077, 0.1098,  ..., 0.1016, 0.1115, 0.1027]],\n",
       "\n",
       "         [[0.1033, 0.1086, 0.1152,  ..., 0.1198, 0.1226, 0.1171],\n",
       "          [0.0000, 0.1063, 0.1071,  ..., 0.1085, 0.1151, 0.1053],\n",
       "          [0.1035, 0.1141, 0.1124,  ..., 0.1190, 0.1117, 0.1118],\n",
       "          ...,\n",
       "          [0.1157, 0.1042, 0.1125,  ..., 0.1115, 0.1076, 0.1129],\n",
       "          [0.1126, 0.1127, 0.1177,  ..., 0.1063, 0.1033, 0.1103],\n",
       "          [0.1181, 0.1129, 0.1153,  ..., 0.1152, 0.1173, 0.1064]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0000, 0.1223, 0.1075,  ..., 0.1020, 0.1204, 0.1105],\n",
       "          [0.1077, 0.1140, 0.1196,  ..., 0.1210, 0.0000, 0.1036],\n",
       "          [0.1012, 0.0000, 0.0993,  ..., 0.1214, 0.1055, 0.1047],\n",
       "          ...,\n",
       "          [0.1117, 0.1080, 0.1113,  ..., 0.0000, 0.1024, 0.1143],\n",
       "          [0.1114, 0.1114, 0.1099,  ..., 0.1086, 0.1109, 0.1253],\n",
       "          [0.1071, 0.1081, 0.1089,  ..., 0.1065, 0.1243, 0.1206]],\n",
       "\n",
       "         [[0.1203, 0.1120, 0.1102,  ..., 0.1055, 0.1007, 0.1090],\n",
       "          [0.1207, 0.1117, 0.1185,  ..., 0.0000, 0.1104, 0.1105],\n",
       "          [0.1076, 0.1196, 0.1135,  ..., 0.1067, 0.1027, 0.0990],\n",
       "          ...,\n",
       "          [0.1071, 0.1117, 0.1121,  ..., 0.1057, 0.0000, 0.1072],\n",
       "          [0.1111, 0.1139, 0.1065,  ..., 0.1079, 0.1120, 0.1078],\n",
       "          [0.1174, 0.1134, 0.1143,  ..., 0.1159, 0.1236, 0.1084]],\n",
       "\n",
       "         [[0.1058, 0.1143, 0.1148,  ..., 0.1146, 0.1126, 0.1103],\n",
       "          [0.1163, 0.0000, 0.1115,  ..., 0.1064, 0.1155, 0.1123],\n",
       "          [0.1048, 0.1062, 0.1180,  ..., 0.1046, 0.1224, 0.1066],\n",
       "          ...,\n",
       "          [0.1004, 0.1054, 0.1217,  ..., 0.1094, 0.1133, 0.1064],\n",
       "          [0.1025, 0.1108, 0.1196,  ..., 0.1311, 0.1038, 0.1229],\n",
       "          [0.1075, 0.1119, 0.1106,  ..., 0.0000, 0.1143, 0.1236]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.1165, 0.1082, 0.1099,  ..., 0.1126, 0.1086, 0.1000],\n",
       "          [0.1053, 0.0000, 0.1152,  ..., 0.1064, 0.1025, 0.0000],\n",
       "          [0.1134, 0.0000, 0.1118,  ..., 0.0989, 0.0000, 0.1279],\n",
       "          ...,\n",
       "          [0.1078, 0.1208, 0.1032,  ..., 0.1052, 0.1084, 0.0000],\n",
       "          [0.1215, 0.0000, 0.1094,  ..., 0.1050, 0.1212, 0.1060],\n",
       "          [0.1145, 0.1051, 0.1125,  ..., 0.0000, 0.1085, 0.1147]],\n",
       "\n",
       "         [[0.1129, 0.1110, 0.0000,  ..., 0.1051, 0.1044, 0.1126],\n",
       "          [0.1102, 0.1109, 0.1123,  ..., 0.1198, 0.1064, 0.1125],\n",
       "          [0.1114, 0.1048, 0.1132,  ..., 0.1063, 0.1177, 0.1058],\n",
       "          ...,\n",
       "          [0.1072, 0.1163, 0.1128,  ..., 0.1087, 0.1112, 0.1106],\n",
       "          [0.1082, 0.0992, 0.1016,  ..., 0.1048, 0.1131, 0.0000],\n",
       "          [0.1093, 0.1055, 0.1105,  ..., 0.1019, 0.1122, 0.0000]],\n",
       "\n",
       "         [[0.1060, 0.1095, 0.1140,  ..., 0.1193, 0.1238, 0.0000],\n",
       "          [0.1126, 0.1105, 0.1142,  ..., 0.1163, 0.1192, 0.1003],\n",
       "          [0.1090, 0.1094, 0.1115,  ..., 0.0000, 0.1116, 0.1095],\n",
       "          ...,\n",
       "          [0.1140, 0.1060, 0.1122,  ..., 0.1096, 0.1123, 0.1073],\n",
       "          [0.1131, 0.0000, 0.1178,  ..., 0.1101, 0.1035, 0.1127],\n",
       "          [0.1161, 0.1150, 0.1118,  ..., 0.0000, 0.1115, 0.1113]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.1068, 0.0000, 0.1124,  ..., 0.1040, 0.1234, 0.1184],\n",
       "          [0.1103, 0.1105, 0.1155,  ..., 0.1150, 0.1104, 0.1112],\n",
       "          [0.1028, 0.1099, 0.1034,  ..., 0.1260, 0.1040, 0.1065],\n",
       "          ...,\n",
       "          [0.1099, 0.1117, 0.1083,  ..., 0.0000, 0.1062, 0.1152],\n",
       "          [0.1098, 0.1140, 0.1176,  ..., 0.1126, 0.1122, 0.1194],\n",
       "          [0.0998, 0.1208, 0.1082,  ..., 0.1099, 0.1185, 0.1161]],\n",
       "\n",
       "         [[0.1204, 0.1154, 0.1116,  ..., 0.0991, 0.1081, 0.1080],\n",
       "          [0.0000, 0.0000, 0.1099,  ..., 0.1032, 0.1104, 0.0000],\n",
       "          [0.1144, 0.1220, 0.1089,  ..., 0.1060, 0.1040, 0.0989],\n",
       "          ...,\n",
       "          [0.1084, 0.1072, 0.1062,  ..., 0.1106, 0.1194, 0.1061],\n",
       "          [0.1109, 0.1160, 0.0000,  ..., 0.1073, 0.1067, 0.1040],\n",
       "          [0.1216, 0.1090, 0.1138,  ..., 0.1177, 0.1240, 0.1043]],\n",
       "\n",
       "         [[0.1012, 0.1107, 0.1193,  ..., 0.1209, 0.1096, 0.1094],\n",
       "          [0.1100, 0.1137, 0.0000,  ..., 0.1142, 0.1154, 0.1075],\n",
       "          [0.1077, 0.1085, 0.1164,  ..., 0.1059, 0.1276, 0.1000],\n",
       "          ...,\n",
       "          [0.1059, 0.1041, 0.1291,  ..., 0.1121, 0.1112, 0.1085],\n",
       "          [0.1046, 0.1068, 0.1208,  ..., 0.1302, 0.1085, 0.1234],\n",
       "          [0.1139, 0.0000, 0.1108,  ..., 0.1091, 0.1092, 0.1209]]],\n",
       "\n",
       "\n",
       "        [[[0.1117, 0.1110, 0.1158,  ..., 0.1105, 0.1107, 0.1034],\n",
       "          [0.1065, 0.1145, 0.1139,  ..., 0.1083, 0.1053, 0.1110],\n",
       "          [0.1139, 0.1081, 0.0000,  ..., 0.0959, 0.1025, 0.1269],\n",
       "          ...,\n",
       "          [0.1044, 0.1180, 0.1063,  ..., 0.1128, 0.1060, 0.1106],\n",
       "          [0.1187, 0.1064, 0.1120,  ..., 0.1075, 0.1169, 0.1055],\n",
       "          [0.1178, 0.1020, 0.0000,  ..., 0.0000, 0.0986, 0.1128]],\n",
       "\n",
       "         [[0.1139, 0.1111, 0.1103,  ..., 0.1131, 0.1015, 0.1142],\n",
       "          [0.1090, 0.1099, 0.0000,  ..., 0.1208, 0.1097, 0.1095],\n",
       "          [0.1104, 0.1013, 0.1078,  ..., 0.0000, 0.0000, 0.1052],\n",
       "          ...,\n",
       "          [0.1064, 0.1160, 0.1145,  ..., 0.1121, 0.1111, 0.1132],\n",
       "          [0.1094, 0.1111, 0.1013,  ..., 0.1081, 0.0000, 0.0963],\n",
       "          [0.1092, 0.1121, 0.0000,  ..., 0.1069, 0.1082, 0.1064]],\n",
       "\n",
       "         [[0.1048, 0.1069, 0.1189,  ..., 0.1192, 0.1312, 0.1152],\n",
       "          [0.0000, 0.1070, 0.1063,  ..., 0.1148, 0.1176, 0.1006],\n",
       "          [0.1081, 0.1206, 0.1115,  ..., 0.1154, 0.1123, 0.1130],\n",
       "          ...,\n",
       "          [0.1168, 0.1078, 0.1127,  ..., 0.1055, 0.1165, 0.1134],\n",
       "          [0.1160, 0.1151, 0.1204,  ..., 0.1083, 0.1121, 0.1150],\n",
       "          [0.1150, 0.0000, 0.1184,  ..., 0.1117, 0.1179, 0.1104]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0000, 0.1197, 0.1043,  ..., 0.1052, 0.1225, 0.1174],\n",
       "          [0.0000, 0.1159, 0.1157,  ..., 0.1157, 0.1054, 0.1139],\n",
       "          [0.1066, 0.1124, 0.1047,  ..., 0.1208, 0.1070, 0.1040],\n",
       "          ...,\n",
       "          [0.1071, 0.1110, 0.1181,  ..., 0.1128, 0.0990, 0.1201],\n",
       "          [0.1055, 0.1091, 0.1090,  ..., 0.1069, 0.1115, 0.1215],\n",
       "          [0.1018, 0.1167, 0.1106,  ..., 0.0000, 0.1196, 0.1174]],\n",
       "\n",
       "         [[0.1161, 0.1096, 0.1131,  ..., 0.1052, 0.0000, 0.1069],\n",
       "          [0.1230, 0.1114, 0.1172,  ..., 0.1011, 0.1106, 0.1134],\n",
       "          [0.1135, 0.1161, 0.1175,  ..., 0.1159, 0.1023, 0.1015],\n",
       "          ...,\n",
       "          [0.1091, 0.1058, 0.1124,  ..., 0.0000, 0.1210, 0.1006],\n",
       "          [0.1078, 0.1124, 0.1075,  ..., 0.0000, 0.1058, 0.1097],\n",
       "          [0.1150, 0.1126, 0.1171,  ..., 0.1206, 0.1176, 0.1071]],\n",
       "\n",
       "         [[0.1029, 0.1135, 0.1131,  ..., 0.1142, 0.1102, 0.1107],\n",
       "          [0.1154, 0.1094, 0.1154,  ..., 0.1136, 0.1153, 0.1145],\n",
       "          [0.1035, 0.1084, 0.1120,  ..., 0.1043, 0.1235, 0.1049],\n",
       "          ...,\n",
       "          [0.1065, 0.0996, 0.1178,  ..., 0.1117, 0.1134, 0.1027],\n",
       "          [0.1059, 0.0000, 0.1142,  ..., 0.1274, 0.1110, 0.1214],\n",
       "          [0.1096, 0.1118, 0.1100,  ..., 0.1119, 0.1119, 0.1250]]],\n",
       "\n",
       "\n",
       "        [[[0.1134, 0.1141, 0.1148,  ..., 0.1068, 0.1088, 0.0000],\n",
       "          [0.1129, 0.1144, 0.1108,  ..., 0.1069, 0.1025, 0.1075],\n",
       "          [0.1086, 0.1070, 0.0000,  ..., 0.1070, 0.0000, 0.1227],\n",
       "          ...,\n",
       "          [0.0000, 0.1228, 0.1089,  ..., 0.1085, 0.1106, 0.1108],\n",
       "          [0.1267, 0.1051, 0.1087,  ..., 0.1097, 0.1129, 0.1011],\n",
       "          [0.1136, 0.1078, 0.1146,  ..., 0.1102, 0.1016, 0.1131]],\n",
       "\n",
       "         [[0.1106, 0.1128, 0.1143,  ..., 0.0000, 0.1070, 0.1064],\n",
       "          [0.1079, 0.1173, 0.1191,  ..., 0.1194, 0.0000, 0.1098],\n",
       "          [0.1100, 0.1000, 0.1159,  ..., 0.1120, 0.0000, 0.1058],\n",
       "          ...,\n",
       "          [0.1052, 0.1153, 0.0000,  ..., 0.1136, 0.1105, 0.1128],\n",
       "          [0.1080, 0.1027, 0.1057,  ..., 0.1082, 0.1084, 0.1009],\n",
       "          [0.1152, 0.1145, 0.1117,  ..., 0.1097, 0.1094, 0.0000]],\n",
       "\n",
       "         [[0.1111, 0.1121, 0.1235,  ..., 0.1234, 0.1278, 0.1151],\n",
       "          [0.1054, 0.1088, 0.1117,  ..., 0.1117, 0.1167, 0.1026],\n",
       "          [0.1070, 0.1126, 0.1113,  ..., 0.1147, 0.1171, 0.1103],\n",
       "          ...,\n",
       "          [0.1157, 0.1035, 0.1099,  ..., 0.1193, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1154, 0.1195,  ..., 0.1026, 0.1092, 0.1123],\n",
       "          [0.1131, 0.0000, 0.0000,  ..., 0.1162, 0.1160, 0.1110]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0000, 0.1195, 0.1041,  ..., 0.1029, 0.1231, 0.1222],\n",
       "          [0.1067, 0.1087, 0.1186,  ..., 0.1198, 0.1090, 0.1125],\n",
       "          [0.1052, 0.1145, 0.1006,  ..., 0.1145, 0.1066, 0.1072],\n",
       "          ...,\n",
       "          [0.1081, 0.1108, 0.1121,  ..., 0.1138, 0.1019, 0.1168],\n",
       "          [0.0000, 0.1108, 0.0000,  ..., 0.1119, 0.1139, 0.1187],\n",
       "          [0.1041, 0.1093, 0.1113,  ..., 0.1022, 0.1227, 0.1126]],\n",
       "\n",
       "         [[0.1193, 0.1113, 0.1083,  ..., 0.1052, 0.0000, 0.1064],\n",
       "          [0.1237, 0.0000, 0.1120,  ..., 0.1035, 0.1095, 0.1110],\n",
       "          [0.1156, 0.1207, 0.1118,  ..., 0.1103, 0.1038, 0.1014],\n",
       "          ...,\n",
       "          [0.1135, 0.1102, 0.1209,  ..., 0.0000, 0.1203, 0.1081],\n",
       "          [0.0000, 0.1127, 0.1059,  ..., 0.0000, 0.1123, 0.1122],\n",
       "          [0.1170, 0.1089, 0.1148,  ..., 0.1246, 0.1219, 0.1117]],\n",
       "\n",
       "         [[0.0000, 0.1115, 0.1117,  ..., 0.1186, 0.0000, 0.1139],\n",
       "          [0.1127, 0.1070, 0.1104,  ..., 0.1075, 0.1179, 0.1143],\n",
       "          [0.0000, 0.1084, 0.0000,  ..., 0.1123, 0.1202, 0.1016],\n",
       "          ...,\n",
       "          [0.1013, 0.0000, 0.1105,  ..., 0.1082, 0.1093, 0.1018],\n",
       "          [0.1028, 0.1120, 0.1104,  ..., 0.1237, 0.1136, 0.1240],\n",
       "          [0.1099, 0.0000, 0.1065,  ..., 0.1113, 0.1120, 0.1217]]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_layer.attn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
