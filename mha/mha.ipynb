{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "   \"torch\",\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, List\n",
    "import torch\n",
    "from torch import nn\n",
    "from labml import tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- regarding `def foward(x)`:\n",
    "    - Input has shape `[seq_len, batch_size, d_model]` or `[batch_size, d_model]`. We apply the linear transformation to the last dimension and split that into the heads.\n",
    "- Output shape `return x` has `[seq_len, batch_size, heads, d_k] or [batch_size, heads, d_model]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareForMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, heads: int, d_k: int, bias: bool):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, heads * d_k, bias=bias)\n",
    "        self.heads = heads\n",
    "        self.d_k = d_k\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        head_shape = x.shape[:-1]\n",
    "        x = self.linear(x) # Linear transformation\n",
    "        x = x.view(*head_shape, self.heads, self.d_k) # Split last dimensions into heads\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention Module\n",
    "- this computes scaled multi-head attention for a given `query`, `key` and `value` vectors\n",
    "- In simple terms: It finds the key that matches the query, and gets the value of those keys\n",
    "- It uses dot product of query and key as the indicator of how matching they are\n",
    "- Before taking the softmax the dot-products are scaled by $\\frac{1}{\\sqrt{d_k}}$\n",
    "- This is done to avoid large dot-product values causing softmax to give very small gradients when $d_k$ is large\n",
    "- Softmax is calculated along the axis of the sequence (or time)\n",
    "- regarding `def get_scores(query, key)`:\n",
    "    - This calculates $QK^T$\n",
    "    - but this method can also be overriden for other variations like relative attention\n",
    "- regarding `def prepare_mask(mask, query_shape, key_shape)`:\n",
    "    - mask has shape `[seq_len_q, seq_len_k, batch_size]` , where first dimension is the query dimension. If the query dimension is equal to 1 it will be broadcasted\n",
    "    - resulting mask has shape `[seq_len_q, seq_len_k, batch_size, heads]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // heads # Number of features per head\n",
    "        self.heads = heads\n",
    "        # These transform the query, key and value vectors for multi-head attention\n",
    "        self.query = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n",
    "        self.key = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n",
    "        self.value = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=True)\n",
    "        self.softmax = nn.Softmax(dim=1) # Softmax for attention along the time dimension of 'key'\n",
    "        self.output = nn.Linear(d_model, d_model) # Output layer\n",
    "        self.dropout = nn.Dropout(dropout_prob) # Dropout\n",
    "        self.scale = 1 / math.sqrt(self.d_k) # Scaling factor before the softmax\n",
    "        # We store the attentions so that it can be used for logging, or other computations if needed\n",
    "        self.attn = None\n",
    "\n",
    "    def get_scores(self, query: torch.Tensor, key: torch.Tensor):\n",
    "        return torch.einsum('ibhd,jbhd->ijbh', query, key)\n",
    "    \n",
    "    def prepare_mask(self, mask: torch.Tensor, query_shape: List[int], key_shape: List[int]):\n",
    "        assert mask.shape[0] == 1 or mask.shape[0] == query_shape[0]\n",
    "        assert mask.shape[1] == key_shape[0]\n",
    "        assert mask.shape[2] == 1 or mask.shape[2] == query_shape[1]\n",
    "\n",
    "        mask = mask.unsqueeze(-1) # Same mask applied to all heads\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, *, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        seq_len, batch_size, _ = query.shape # query , key and value have shape [seq_len, batch_size, d_model]\n",
    "        if mask is not None:\n",
    "            mask = self.prepare_mask(mask, query.shape, key.shape)\n",
    "        \n",
    "        # Prepare query, key and value for attention computation. These will then have shape [seq_len, batch_size, heads, d_k]\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "\n",
    "        # Compute the attention scores QK^T. This gives a tensor of shape [seq_len, seq_len, batch_size, heads]\n",
    "        scores = self.get_scores(query, key)\n",
    "        scores *= self.scale # Scaling the scores\n",
    "\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Softmax attention along the key sequence dimension\n",
    "        attn = self.softmax(scores)\n",
    "\n",
    "        # Save attentions if debugging\n",
    "        tracker.debug('attn', attn)\n",
    "\n",
    "        # Apply Dropout\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        x = torch.einsum(\"ijbh,jbhd->ibhd\", attn, value)\n",
    "\n",
    "        # Save attentions for any other calculations\n",
    "        self.attn = attn.detach()\n",
    "\n",
    "        # Concatenate multiple heads\n",
    "        x = x.reshape(seq_len, batch_size, -1)\n",
    "\n",
    "        return self.output(x) # Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 32\n",
    "dropout_prob = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = MultiHeadAttention(heads=heads, d_model=d_model, dropout_prob=dropout_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = torch.rand(seq_len, batch_size, d_model) # Shape: [seq_len, batch_size, d_model]\n",
    "key = torch.rand(seq_len, batch_size, d_model) # Shape: [seq_len, batch_size, d_model]\n",
    "value = torch.rand(seq_len, batch_size, d_model) # Shape: [seq_len, batch_size, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones(1, seq_len, seq_len) # Optional mask (shape: [batch_size, seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (32) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m attention_layer(query\u001b[38;5;241m=\u001b[39mquery, key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39mvalue, mask\u001b[38;5;241m=\u001b[39mmask)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 60\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Apply mask\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     scores \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mmasked_fill(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Softmax attention along the key sequence dimension\u001b[39;00m\n\u001b[1;32m     63\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(scores)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (32) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "output = attention_layer(query=query, key=key, value=value, mask=mask) # forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
