{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "   \"torch\",\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, List\n",
    "import torch\n",
    "from torch import nn\n",
    "from labml import tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- regarding `def foward(x)`:\n",
    "    - Input has shape `[seq_len, batch_size, d_model]` or `[batch_size, d_model]`. We apply the linear transformation to the last dimension and split that into the heads.\n",
    "- Output shape `return x` has `[seq_len, batch_size, heads, d_k] or [batch_size, heads, d_model]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareForMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, heads: int, d_k: int, bias: bool):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, heads * d_k, bias=bias)\n",
    "        self.heads = heads\n",
    "        self.d_k = d_k\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        head_shape = x.shape[:-1]\n",
    "        x = self.linear(x) # Linear transformation\n",
    "        x = x.view(*head_shape, self.heads, self.d_k) # Split last dimensions into heads\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention Module\n",
    "- this computes scaled multi-head attention for a given `query`, `key` and `value` vectors\n",
    "- In simple terms: It finds the key that matches the query, and gets the value of those keys\n",
    "- It uses dot product of query and key as the indicator of how matching they are\n",
    "- Before taking the softmax the dot-products are scaled by $\\frac{1}{\\sqrt{d_k}}$\n",
    "- This is done to avoid large dot-product values causing softmax to give very small gradients when $d_k$ is large\n",
    "- Softmax is calculated along the axis of the sequence (or time)\n",
    "- regarding `def get_scores(query, key)`:\n",
    "    - This calculates $QK^T$\n",
    "    - but this method can also be overriden for other variations like relative attention\n",
    "- regarding `def prepare_mask(mask, query_shape, key_shape)`:\n",
    "    - mask has shape `[seq_len_q, seq_len_k, batch_size]` , where first dimension is the query dimension. If the query dimension is equal to 1 it will be broadcasted\n",
    "    - resulting mask has shape `[seq_len_q, seq_len_k, batch_size, heads]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // heads # Number of features per head\n",
    "        self.heads = heads\n",
    "        # These transform the query, key and value vectors for multi-head attention\n",
    "        self.query = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n",
    "        self.key = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n",
    "        self.value = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=True)\n",
    "        self.softmax = nn.Softmax(dim=1) # Softmax for attention along the time dimension of 'key'\n",
    "        self.output = nn.Linear(d_model, d_model) # Output layer\n",
    "        self.dropout = nn.Dropout(dropout_prob) # Dropout\n",
    "        self.scale = 1 / math.sqrt(self.d_k) # Scaling factor before the softmax\n",
    "        # We store the attentions so that it can be used for logging, or other computations if needed\n",
    "        self.attn = None\n",
    "\n",
    "    def get_scores(self, query: torch.Tensor, key: torch.Tensor):\n",
    "        return torch.einsum('ibhd,jbhd->ijbh', query, key)\n",
    "    \n",
    "    def prepare_mask(self, mask: torch.Tensor, query_shape: List[int], key_shape: List[int]):\n",
    "        assert mask.shape[0] == 1 or mask.shape[0] == query_shape[0]\n",
    "        assert mask.shape[1] == key_shape[0]\n",
    "        assert mask.shape[2] == 1 or mask.shape[2] == query_shape[1]\n",
    "\n",
    "        mask = mask.unsqueeze(-1) # Same mask applied to all heads\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, *, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        seq_len, batch_size, _ = query.shape # query , key and value have shape [seq_len, batch_size, d_model]\n",
    "        if mask is not None:\n",
    "            mask = self.prepare_mask(mask, query.shape, key.shape)\n",
    "        \n",
    "        # Prepare query, key and value for attention computation. These will then have shape [seq_len, batch_size, heads, d_k]\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "\n",
    "        # Compute the attention scores QK^T. This gives a tensor of shape [seq_len, seq_len, batch_size, heads]\n",
    "        scores = self.get_scores(query, key)\n",
    "        scores *= self.scale # Scaling the scores\n",
    "\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Softmax attention along the key sequence dimension\n",
    "        attn = self.softmax(scores)\n",
    "\n",
    "        # Save attentions if debugging\n",
    "        tracker.debug('attn', attn)\n",
    "\n",
    "        # Apply Dropout\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        x = torch.einsum(\"ijbh,jbhd->ibhd\", attn, value)\n",
    "\n",
    "        # Save attentions for any other calculations\n",
    "        self.attn = attn.detach()\n",
    "\n",
    "        # Concatenate multiple heads\n",
    "        x = x.reshape(seq_len, batch_size, -1)\n",
    "\n",
    "        return self.output(x) # Output layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
