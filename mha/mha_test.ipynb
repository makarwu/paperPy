{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer experiment\n",
    "- This trains a simple transformer with multi headed attention introduced in Attention is all You Need on an NLP auto-regression task with the Tiny Shakespeare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install labml-nn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from labml import experiment\n",
    "from labml.configs import option\n",
    "from labml_nn.transformers.basic.autoregressive_experiment import Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.create(name=\"transformer\", writers={'screen'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.configs(conf, {\n",
    "    # Use character level tokenizer\n",
    "    'tokenizer': 'character',\n",
    "    # Prompt separator is blank\n",
    "    'prompt_separator': '',\n",
    "    # Starting prompt for sampling\n",
    "    'prompt': 'It is ',\n",
    "    # Use Tiny Shakespeare dataset\n",
    "    'text': 'tiny_shakespeare',\n",
    "\n",
    "    # Use a context size of $256$\n",
    "    'seq_len': 512,\n",
    "    # Train for 32 epochs\n",
    "    'epochs': 32,\n",
    "    # Batch size $32$\n",
    "    'batch_size': 16,\n",
    "    # Switch between training and validation for 10 times\n",
    "    # per epoch\n",
    "    'inner_iterations': 10,\n",
    "\n",
    "    # Model size\n",
    "    'd_model': 256,\n",
    "    'transformer.n_heads': 16,\n",
    "    'transformer.ffn.d_ff': 1024,\n",
    "\n",
    "    # Use [Noam optimizer](../../optimizers/noam.html)\n",
    "    'optimizer.optimizer': 'Noam',\n",
    "    'optimizer.learning_rate': 1.,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.add_pytorch_models({'model': conf.model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with experiment.start():\n",
    "    conf.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
