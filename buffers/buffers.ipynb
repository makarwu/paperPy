{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUjy0UuzNjLJ"
      },
      "source": [
        "### Understanding PyTorch Buffers\n",
        "- Buffers are tensor attributes associated with a PyTorch module or model similar to parameters, but unlike parameters, buffers are not updated during training\n",
        "- they are particularly useful when dealing with GPU computations, as they need to be transferred between devices (from CPU to GPU) alongside the model's parameters\n",
        "- Unlike parameters, buffers do not require gradient computation, but they still need to be on the correct device to ensure all computations are performed correctly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12QN1g0yNjLM"
      },
      "source": [
        "### An example without buffers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u1gm4kioNjLM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CausalAttentionWithoutBuffers(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length,\n",
        "                 dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1, 2)\n",
        "        attn_scores.masked_fill_(\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmcxuukuNjLN"
      },
      "source": [
        "- Run the module on some example data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn0mgOY7NjLN",
        "outputId": "b13fbeb0-e8d4-4440-b941-c3dae1e0404b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]],\n",
            "\n",
            "        [[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "context_length = batch.shape[1]\n",
        "d_in = inputs.shape[1]\n",
        "d_out = 2\n",
        "\n",
        "ca_without_buffer = CausalAttentionWithoutBuffers(d_in, d_out, context_length, 0.0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    context_vecs = ca_without_buffer(batch)\n",
        "print(context_vecs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04q2wH-ENjLN"
      },
      "source": [
        "However, when training LLMs, we typically use GPUs to accelerate the process. Therefore, let's transfer the `CasualAttentionWithoutBuffers` module onto a GPU device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMc29njSNjLN",
        "outputId": "325fda50-2f2d-46b9-b8f6-b133b2b8cf65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine has GPU: True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CausalAttentionWithoutBuffers(\n",
              "  (W_query): Linear(in_features=3, out_features=2, bias=False)\n",
              "  (W_key): Linear(in_features=3, out_features=2, bias=False)\n",
              "  (W_value): Linear(in_features=3, out_features=2, bias=False)\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "print(\"Machine has GPU:\", torch.cuda.is_available())\n",
        "\n",
        "batch = batch.to(\"cuda\")\n",
        "ca_without_buffer.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  context_vecs = ca_without_buffer(batch)\n",
        "print(context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "KRa_L5qKjYZu",
        "outputId": "c393c437-6e57-4afb-e50c-dc05546855a6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "expected self and mask to be on the same device, but got mask on cpu and self on cuda:0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4e0b3060b21b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mcontext_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mca_without_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-cf1dad0dd611>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mattn_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueries\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         attn_scores.masked_fill_(\n\u001b[0m\u001b[1;32m     24\u001b[0m             self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n\u001b[1;32m     25\u001b[0m         attn_weights = torch.softmax(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected self and mask to be on the same device, but got mask on cpu and self on cuda:0"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the code resulted in an error, as expected. It seems like we attempted a matrix multiplication between a tensor on a GPU and a tensor on a CPU. But we moved the module to the GPU?"
      ],
      "metadata": {
        "id": "U1MTcpyEj5ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"W_query.device: \", ca_without_buffer.W_query.weight.device)\n",
        "print(\"mask.device: \", ca_without_buffer.mask.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je3dlHY0j1gr",
        "outputId": "cd19cff7-3028-45f8-dcfe-61684e6bb915"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_query.device:  cuda:0\n",
            "mask.device:  cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(ca_without_buffer.mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp96sbFrkZrk",
        "outputId": "b8803704-8667-4c7b-f828-4d56367b2373"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, mask was not moved onto the GPU. That's because it's not a PyTorch parameter like the weights (e.g., W_query.weight)\n",
        "That means we have to manually move the mask to the gpu via `.to(\"cuda\")`"
      ],
      "metadata": {
        "id": "IrzO3HQqkpgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ca_without_buffer.mask = ca_without_buffer.mask.to(\"cuda\")\n",
        "print(\"mask.device: \", ca_without_buffer.mask.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA_NpRxKkkSU",
        "outputId": "2e697025-0875-487d-c17f-702aba169a91"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mask.device:  cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  context_vecs = ca_without_buffer(batch)\n",
        "print(context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqC4dNPsmyGt",
        "outputId": "da23920b-42bd-4aff-c552-b14bbe3efb0f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]],\n",
            "\n",
            "        [[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, remembering to move individual tensors to the GPU can be tendious. As we can see, it's easier to use `register_buffer` to register the mask as a buffer."
      ],
      "metadata": {
        "id": "r8_sSnqtnpFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### An example with buffers"
      ],
      "metadata": {
        "id": "37A5Xv_ooAO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CausalAttentionWithBuffer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length,\n",
        "                 dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Old:\n",
        "        # self.mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "\n",
        "        # New:\n",
        "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1, 2)\n",
        "        attn_scores.masked_fill_(\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "CkURQpBlnj7k"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ca_with_buffer = CausalAttentionWithBuffer(d_in, d_out, context_length, 0.0)\n",
        "ca_with_buffer.to(\"cuda\")\n",
        "\n",
        "print(\"W_query.device:\", ca_with_buffer.W_query.weight.device)\n",
        "print(\"mask.device:\", ca_with_buffer.mask.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5p4nUQi_oIEC",
        "outputId": "34029f9b-1039-4ee2-f0df-959e484e5eac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_query.device: cuda:0\n",
            "mask.device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    context_vecs = ca_with_buffer(batch)\n",
        "\n",
        "print(context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O26OjUQboLPi",
        "outputId": "048bbe2f-5fd4-43cc-e47b-b753b320273e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.4772, 0.1063],\n",
            "         [0.5891, 0.3257],\n",
            "         [0.6202, 0.3860],\n",
            "         [0.5478, 0.3589],\n",
            "         [0.5321, 0.3428],\n",
            "         [0.5077, 0.3493]],\n",
            "\n",
            "        [[0.4772, 0.1063],\n",
            "         [0.5891, 0.3257],\n",
            "         [0.6202, 0.3860],\n",
            "         [0.5478, 0.3589],\n",
            "         [0.5321, 0.3428],\n",
            "         [0.5077, 0.3493]]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Buffers and `state_dict`\n",
        "- Another advantage of PyTorch buffers, over regular tensors, is that they get included in a model's `state_dict`"
      ],
      "metadata": {
        "id": "K279fqfEofRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ca_without_buffer.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dbExkYUoNwW",
        "outputId": "ee1c499e-e437-4173-e02f-849db76b8f42"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('W_query.weight',\n",
              "              tensor([[-0.2354,  0.0191, -0.2867],\n",
              "                      [ 0.2177, -0.4919,  0.4232]], device='cuda:0')),\n",
              "             ('W_key.weight',\n",
              "              tensor([[-0.4196, -0.4590, -0.3648],\n",
              "                      [ 0.2615, -0.2133,  0.2161]], device='cuda:0')),\n",
              "             ('W_value.weight',\n",
              "              tensor([[-0.4900, -0.3503, -0.2120],\n",
              "                      [-0.1135, -0.4404,  0.3780]], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ca_with_buffer.state_dict() # mask is in there"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_tNFN0MovpN",
        "outputId": "03c22f6b-660c-4703-d49f-c24e413a7c91"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('mask',\n",
              "              tensor([[0., 1., 1., 1., 1., 1.],\n",
              "                      [0., 0., 1., 1., 1., 1.],\n",
              "                      [0., 0., 0., 1., 1., 1.],\n",
              "                      [0., 0., 0., 0., 1., 1.],\n",
              "                      [0., 0., 0., 0., 0., 1.],\n",
              "                      [0., 0., 0., 0., 0., 0.]], device='cuda:0')),\n",
              "             ('W_query.weight',\n",
              "              tensor([[-0.1362,  0.1853,  0.4083],\n",
              "                      [ 0.1076,  0.1579,  0.5573]], device='cuda:0')),\n",
              "             ('W_key.weight',\n",
              "              tensor([[-0.2604,  0.1829, -0.2569],\n",
              "                      [ 0.4126,  0.4611, -0.5323]], device='cuda:0')),\n",
              "             ('W_value.weight',\n",
              "              tensor([[ 0.4929,  0.2757,  0.2516],\n",
              "                      [ 0.2377,  0.4800, -0.0762]], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A `state_dict` is useful when saving and loading trained PyTorch models"
      ],
      "metadata": {
        "id": "W0SRVP6wo5VI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ca_with_buffer.mask[ca_with_buffer.mask == 1.] = 2.\n",
        "ca_with_buffer.mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Did3wx4oyU8",
        "outputId": "66aac4fe-5ecb-41c9-8c86-2acc38a21a9b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 2., 2., 2., 2., 2.],\n",
              "        [0., 0., 2., 2., 2., 2.],\n",
              "        [0., 0., 0., 2., 2., 2.],\n",
              "        [0., 0., 0., 0., 2., 2.],\n",
              "        [0., 0., 0., 0., 0., 2.],\n",
              "        [0., 0., 0., 0., 0., 0.]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we load the model, we can see that the mask is fully restored with the modified value:"
      ],
      "metadata": {
        "id": "J0kKhtCOpRAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(ca_with_buffer.state_dict(), \"model.pth\")\n",
        "\n",
        "new_ca_with_buffer = CausalAttentionWithBuffer(d_in, d_out, context_length, 0.0)\n",
        "new_ca_with_buffer.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "new_ca_with_buffer.mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KEaISQupGlQ",
        "outputId": "c6e8b277-4d04-4a57-f7f5-706eef088ad3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-e59a7053c7b1>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  new_ca_with_buffer.load_state_dict(torch.load(\"model.pth\"))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 2., 2., 2., 2., 2.],\n",
              "        [0., 0., 2., 2., 2., 2.],\n",
              "        [0., 0., 0., 2., 2., 2.],\n",
              "        [0., 0., 0., 0., 2., 2.],\n",
              "        [0., 0., 0., 0., 0., 2.],\n",
              "        [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is not true, if we don't use buffers:"
      ],
      "metadata": {
        "id": "jIDUwF73pYdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ca_without_buffer.mask[ca_without_buffer.mask == 1.] = 2.\n",
        "\n",
        "torch.save(ca_without_buffer.state_dict(), \"model.pth\")\n",
        "\n",
        "new_ca_without_buffer = CausalAttentionWithoutBuffers(d_in, d_out, context_length, 0.0)\n",
        "new_ca_without_buffer.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "new_ca_without_buffer.mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD3cHZyOpKcz",
        "outputId": "a5efda07-22f0-4898-868c-a77ce376b63e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-4ccd7b52646b>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  new_ca_without_buffer.load_state_dict(torch.load(\"model.pth\"))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 1., 1., 1., 1., 1.],\n",
              "        [0., 0., 1., 1., 1., 1.],\n",
              "        [0., 0., 0., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}